{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfbd45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5436e7",
   "metadata": {},
   "source": [
    "# This notebook file splits the data according to the subset dataset wanted\n",
    "## It splits the data into training, and evaluation data\n",
    "## It calculates the amount of data for training and evaluation based on the size of the subset dataset asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c233c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_and_save(url, amount, name, column_names_to_convert):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "    url: url to retrieve data from\n",
    "    amount: amount of data wanted to select overall\n",
    "    name: the dataset name e.g., \"Twitter/Reddit\"\n",
    "    column_names_to_convert: map the text column as named in the dataset to 'text' \n",
    "                             and the label column as named in the dataset to 'label'\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculating the amount of data needed for training and evaluation\n",
    "    # based on the overall amount of data asked (dataset subset size)\n",
    "    cat_sel_tr = int((amount*0.8)/3)\n",
    "    cat_sel_eval = int((amount*0.2)/3)\n",
    "    \n",
    "    dataset = pd.read_csv(url)\n",
    "    dataset = dataset.dropna().reset_index(drop=True) # remove nan data points\n",
    "\n",
    "    #training data selection\n",
    "    positive = dataset[dataset.category == 1][:cat_sel_tr] #for each class\n",
    "    neutral = dataset[dataset.category == 0][:cat_sel_tr]\n",
    "    negative = dataset[dataset.category == -1][:cat_sel_tr]\n",
    "    tr_dataset = pd.concat([positive, neutral, negative]).sample(frac=1).reset_index(drop=True) # add them together\n",
    "    tr_dataset = tr_dataset.rename(columns=column_names_to_convert) #change the column names accordingly\n",
    "    tr_dataset = tr_dataset.astype({'label': int})\n",
    "    new_label_format = tr_dataset['label'].map({-1:0, 0:1, 1:2}) #remapping of [-1, 0, 1] notations to [0, 1, 2]\n",
    "    tr_dataset['label'] = new_label_format\n",
    "    \n",
    "    # eval data selection\n",
    "    positiveval = dataset[dataset.category == 1][cat_sel_tr+200:cat_sel_tr+200+cat_sel_eval]\n",
    "    neutralval = dataset[dataset.category == 0][cat_sel_tr+200:cat_sel_tr+200+cat_sel_eval]\n",
    "    negativeval = dataset[dataset.category == -1][cat_sel_tr+200:cat_sel_tr+200+cat_sel_eval]\n",
    "    val_dataset = pd.concat([positiveval, neutralval, negativeval]).sample(frac=1).reset_index(drop=True)\n",
    "    val_dataset = val_dataset.rename(columns=column_names_to_convert)\n",
    "    val_dataset = val_dataset.astype({'label': int})\n",
    "    new_label_format = val_dataset['label'].map({-1:0, 0:1, 1:2}) #remapping of [-1, 0, 1] notations to [0, 1, 2]\n",
    "    val_dataset['label'] = new_label_format\n",
    "    \n",
    "    tr_dataset.to_csv(f'Data/{name} training_{amount}.csv')\n",
    "    val_dataset.to_csv(f'Data/{name} eval_{amount}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8864e6",
   "metadata": {},
   "source": [
    "(Uncomment and) Run for each dataset size wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0eba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split_and_save('Data/Twitter_Data.csv', 1500, 'Twitter_Data', {'clean_text': 'text', 'category':'label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split_and_save('Data/Reddit_Data.csv', 1500, 'Reddit_Data', {'clean_comment': 'text', 'category':'label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5229e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split_and_save('Data/Twitter_Data.csv', 3750, 'Twitter_Data', {'clean_text': 'text', 'category':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d8d2be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split_and_save('Data/Reddit_Data.csv', 3750, 'Reddit_Data', {'clean_comment': 'text', 'category':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd4866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
