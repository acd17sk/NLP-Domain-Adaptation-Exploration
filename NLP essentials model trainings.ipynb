{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torcheval\n",
    "import logging\n",
    "import warnings\n",
    "import operator\n",
    "import shutil\n",
    "import torch\n",
    "from torcheval.metrics.functional import multiclass_f1_score, multiclass_accuracy, multiclass_precision, multiclass_recall\n",
    "from simpletransformers.classification import ClassificationArgs\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "from warnings import simplefilter\n",
    "from statistics import mean\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c89963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34b88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf596408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_typical_regr_args(MAX_SEQ_LEN, hd=0.05, epochs=3, evaluate_during_training=False, update_params=None):\n",
    "    \"\"\" Initialize the typical regr arguements.\"\"\"\n",
    "    model_args_reg = ClassificationArgs()\n",
    "    model_args_reg.num_train_epochs = epochs\n",
    "    model_args_reg.overwrite_output_dir = True\n",
    "    model_args_reg.save_steps = -1\n",
    "    model_args_reg.save_model_every_epoch = False\n",
    "    model_args_reg.fp16=False\n",
    "    model_args_reg.regression = False\n",
    "    model_args_reg.num_labels = 3\n",
    "    model_args_reg.evaluate_during_training = evaluate_during_training\n",
    "    model_args_reg.max_seq_length = MAX_SEQ_LEN\n",
    "    model_args_reg.train_batch_size = 8\n",
    "    model_args_reg.config = {'hidden_dropout_prob' : hd}\n",
    "    if update_params:\n",
    "        model_args_reg.custom_parameter_groups = [{\"params\" : update_params}]\n",
    "        model_args_reg.train_custom_parameters_only = True\n",
    "    return model_args_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_double_finetune(model_name, model_version, train_df, hd = 0.05, default_lr = 0.00004, evaluate_during_training=False, MAX_SEQ_LEN = 128, typical_args=True):\n",
    "    \"\"\" \n",
    "    Create and double finetune (HeFit) \n",
    "    a model given the train dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    model = ClassificationModel(model_name, model_version, num_labels=3) # initialize to get named params\n",
    "\n",
    "    frozen_parameters_size = len(model.get_named_parameters()) - 4 # 4 for the heads layers\n",
    "\n",
    "    # if evaluate_during_training:\n",
    "    #     train_df, eval_df = train_test_split(train_df, test_size=0.2)\n",
    "    \n",
    "    # get final arguments\n",
    "    # use this to freeze encoder embeddings and not the classifier head\n",
    "    # model.get_named_parameters()[frozen_parameters_size:]\n",
    "    model_args_reg = return_typical_regr_args(MAX_SEQ_LEN,hd=hd, evaluate_during_training=evaluate_during_training, update_params=model.get_named_parameters()[frozen_parameters_size:])\n",
    "    del model\n",
    "\n",
    "    # Create a ClassificationModel with frozen embeddings\n",
    "    model = ClassificationModel(\n",
    "        model_name,\n",
    "        model_version,\n",
    "        num_labels=3,\n",
    "        args=model_args_reg,\n",
    "    )\n",
    "    \n",
    "\n",
    "    if evaluate_during_training:\n",
    "        model.train_model(train_df, eval_df=eval_df)\n",
    "        model = perform_second_stage_finetune(model, train_df, eval_df=eval_df, default_lr=default_lr)\n",
    "    else:\n",
    "        model.train_model(train_df)\n",
    "        model = perform_second_stage_finetune(model, train_df, default_lr=default_lr)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def perform_second_stage_finetune(model, train_df, default_lr, eval_df=None):\n",
    "    \"\"\" \n",
    "    Perform the second stage (unfrozen embeddings) \n",
    "    of the HeFit finetuning procedure.\n",
    "    \"\"\"\n",
    "    # unfreezes embeddings\n",
    "    model.args.custom_parameter_groups = [{\"params\" : model.get_named_parameters()}]\n",
    "    model.args.train_custom_parameters_only = False \n",
    "\n",
    "    # for second stage\n",
    "    model.args.learning_rate = (default_lr/2) # half lr\n",
    "    model.args.num_train_epochs = HeFiT_2nd_stage_epochs # double epochs\n",
    "\n",
    "    \n",
    "    if eval_df == None: # train again (from now checkpoint) with unfrozen embeddings\n",
    "        model.train_model(train_df) \n",
    "    else:\n",
    "        model.train_model(train_df, eval_df=eval_df)\n",
    "    return model\n",
    "\n",
    "def create_and_single_adapter_finetune(model_name, model_version, train_df, adapters_epochs, hd=0.05, default_lr = 0.00004, MAX_SEQ_LEN = 128, evaluate_during_training=False, typical_args=True):\n",
    "    \"\"\" \n",
    "    Create and single finetune using adapters \n",
    "    a model given the train dataframe.\n",
    "    \"\"\"\n",
    "    model_args_reg = return_typical_regr_args(MAX_SEQ_LEN, hd=hd, evaluate_during_training=evaluate_during_training)\n",
    "    model_args_reg.num_train_epochs = adapters_epochs\n",
    "    \n",
    "    model = ClassificationModel(\n",
    "        model_name,\n",
    "        model_version,\n",
    "        num_labels=3,\n",
    "        args=model_args_reg,\n",
    "    )\n",
    "    \n",
    "    #add adapters\n",
    "    model.model.add_adapter(\"CLASSIFICATION_ADAPTERS\")\n",
    "\n",
    "    # Activate the adapters\n",
    "    model.model.train_adapter(\"CLASSIFICATION_ADAPTERS\")\n",
    "\n",
    "    model.train_model(train_df)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_and_single_finetune(model_name, model_version, train_df, sFiT_epochs, hd=0.05, default_lr = 0.00004, MAX_SEQ_LEN = 128, evaluate_during_training=False, typical_args=True):\n",
    "    \"\"\" \n",
    "    Create and single finetune, standard finetune SFIT\n",
    "    a model given the train dataframe.\n",
    "    \"\"\"\n",
    "    model_args_reg = return_typical_regr_args(MAX_SEQ_LEN, hd=hd, evaluate_during_training=evaluate_during_training)\n",
    "    model_args_reg.num_train_epochs = sFiT_epochs\n",
    "\n",
    "    model = ClassificationModel(\n",
    "        model_name,\n",
    "        model_version,\n",
    "        num_labels=3,\n",
    "        args=model_args_reg,\n",
    "    )\n",
    "\n",
    "    model.train_model(train_df)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data, save_name, model):\n",
    "    \"\"\"\n",
    "    evaluates the model With \n",
    "    Accuracy, F1, precision, recall (macro)\n",
    "    saves evaluation in a csv file\n",
    "    \"\"\"\n",
    "    r, _ = model.predict(list(data['text']))\n",
    "    \n",
    "    \n",
    "#     print(multiclass_accuracy(torch.tensor(data['label']), torch.tensor(r), num_classes=3))\n",
    "#     print(multiclass_f1_score(torch.tensor(data['label']), torch.tensor(r), num_classes=3))\n",
    "#     print(multiclass_precision(torch.tensor(data['label']), torch.tensor(r), num_classes=3))\n",
    "#     print(multiclass_recall(torch.tensor(data['label']), torch.tensor(r), num_classes=3))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Accuracy'] = [multiclass_accuracy(torch.tensor(data['label']), torch.tensor(r), num_classes=3)]\n",
    "    df['F1'] = [multiclass_f1_score(torch.tensor(data['label']), torch.tensor(r), num_classes=3, average='macro')]\n",
    "    df['Precision'] = [multiclass_precision(torch.tensor(data['label']), torch.tensor(r), num_classes=3, average='macro')]\n",
    "    df['Recall'] = [multiclass_recall(torch.tensor(data['label']), torch.tensor(r), num_classes=3, average='macro')]\n",
    "    \n",
    "    df.to_csv(save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_all_models(test_pairs, model_name, model_version, lr, MAX_SEQ_LEN, hd=0.05, iterations=5, epochs=3, evaluate_during_training=False, training_mode='hefit'):\n",
    "    \"\"\" finetune(or double) models for all affects regression. \"\"\"\n",
    "    \n",
    "    if isinstance(iterations, int):\n",
    "        for_iter = range(iterations)\n",
    "    else:\n",
    "        for_iter = iterations\n",
    "    \n",
    "    for test_pair in test_pairs:\n",
    "        train_df = pd.read_csv(test_pair[1])\n",
    "        train_df = train_df[['text', 'label']]\n",
    "        test_df = pd.read_csv(test_pair[2])\n",
    "        for i in for_iter:\n",
    "    \n",
    "            if training_mode=='hefit':\n",
    "                model = create_and_double_finetune(model_name = model_name, model_version = model_version[1], train_df = train_df, hd=hd, evaluate_during_training=evaluate_during_training, MAX_SEQ_LEN = MAX_SEQ_LEN)\n",
    "                save_directory = f'{test_pair[0]} Results/HeFit'\n",
    "            elif training_mode=='adapters':\n",
    "                model = create_and_single_adapter_finetune(model_name = model_name, model_version = model_version[1], train_df = train_df, adapters_epochs=epochs, hd=hd, evaluate_during_training=evaluate_during_training, MAX_SEQ_LEN = MAX_SEQ_LEN)\n",
    "                save_directory = f'{test_pair[0]} Results/{epochs} epochs Adapters'\n",
    "            else: #else sfit\n",
    "                model = create_and_single_finetune(model_name = model_name, model_version = model_version[1], train_df = train_df, sFiT_epochs=epochs, hd=hd, evaluate_during_training=evaluate_during_training, MAX_SEQ_LEN = MAX_SEQ_LEN)\n",
    "                save_directory = f'{test_pair[0]} Results/{epochs} epochs SFIT'\n",
    "            \n",
    "            \n",
    "            try:  \n",
    "                os.mkdir(save_directory)  \n",
    "            except OSError as error:  \n",
    "                print('Directory already exists')  \n",
    "            \n",
    "            save_name = f'{save_directory}/{model_name}_{i}_res.csv'\n",
    "            evaluate_model(test_df, save_name, model)\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # shutil.rmtree('outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a0472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sfit_training(epoch_training_list, train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, iterations=5):\n",
    "    for sFit_epochs in epoch_training_list:\n",
    "        train_and_evaluate_all_models(train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, iterations=iterations, epochs=sFit_epochs, evaluate_during_training=False, training_mode='sfit')\n",
    "def run_adapter_training(epoch_training_list, train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, iterations=5):\n",
    "    for adapter_epochs in epoch_training_list:\n",
    "        train_and_evaluate_all_models(train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, iterations=iterations, epochs=adapter_epochs, evaluate_during_training=False, training_mode='adapters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_training_list = [50]\n",
    "it = 5\n",
    "\n",
    "model_name = \"roberta\"\n",
    "model_version = [\"roberta\", \"roberta-base\"]\n",
    "\n",
    "# default_lr = 0.00004\n",
    "default_lr = 4e-5\n",
    "MAX_SEQ_LEN = 128\n",
    "HeFiT_2nd_stage_epochs = 6\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedfabb8",
   "metadata": {},
   "source": [
    "## Twitter Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14488be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dir = f'Data/Twitter_Data training_3750.csv'\n",
    "val_dir = f'Data/Twitter_Data eval_3750.csv'\n",
    "\n",
    "train_test_pairs = [\n",
    "     [f'Twitter_data_3750', tr_dir, val_dir],\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sfit_training([3,4,5,6], train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, \n",
    "                  iterations=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_adapter_training([45, 50, 55, 60], train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, \n",
    "                  iterations=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa48952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hefit\n",
    "train_and_evaluate_all_models(train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, iterations=it, epochs=None, evaluate_during_training=False, training_mode='hefit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b0b9c",
   "metadata": {},
   "source": [
    "## Reddit Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396482e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dir = f'Data/Reddit_Data training_3750.csv'\n",
    "val_dir = f'Data/Reddit_Data eval_3750.csv'\n",
    "\n",
    "train_test_pairs = [\n",
    "     [f'Reddit_data_3750', tr_dir, val_dir],\n",
    " ]\n",
    "\n",
    "MAX_SEQ_LEN = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd585671",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sfit_training([3,4,5,6], train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, \n",
    "                  iterations=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db35615",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_adapter_training([45, 50, 55, 60], train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, \n",
    "                     iterations=it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hefit\n",
    "train_and_evaluate_all_models(train_test_pairs, model_name, model_version, default_lr, MAX_SEQ_LEN, iterations=it, epochs=None, evaluate_during_training=False, training_mode='hefit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5143a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv] *",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
